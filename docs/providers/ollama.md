# Ollama (Local Models)

Ollama is not currently supported as a provider in CCProxy. 

For local model deployment, consider using models available through [OpenRouter](/providers/openrouter) which includes various open-source models that can be run locally through other means.

See the [OpenRouter documentation](/providers/openrouter) for available models and configuration.