# ==================================================
# CCProxy Multi-Provider Configuration
# ==================================================

# ==================================================
# Provider Selection (REQUIRED)
# ==================================================

# Choose your provider: "groq", "openrouter", "openai", "xai", "gemini", "mistral", or "ollama"
PROVIDER=groq

# ==================================================
# Server Configuration
# ==================================================

# Server host (default: 0.0.0.0)
SERVER_HOST=0.0.0.0

# Server port (default: 7187)
SERVER_PORT=7187

# Environment (development, production)
SERVER_ENVIRONMENT=development

# ==================================================
# Groq Provider Configuration
# ==================================================

# Required when PROVIDER=groq
GROQ_API_KEY=your_groq_api_key_here

# Optional Groq settings (defaults shown)
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL=moonshotai/kimi-k2-instruct
GROQ_MAX_TOKENS=16384

# ==================================================
# OpenRouter Provider Configuration  
# ==================================================

# Required when PROVIDER=openrouter
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Optional OpenRouter settings (defaults shown)
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=openai/gpt-4o
OPENROUTER_MAX_TOKENS=4096

# Optional: Site tracking (for OpenRouter analytics)
OPENROUTER_SITE_URL=https://your-site.com
OPENROUTER_SITE_NAME=Your App Name

# ==================================================
# OpenAI Provider Configuration
# ==================================================

# Required when PROVIDER=openai
OPENAI_API_KEY=your_openai_api_key_here

# Optional OpenAI settings (defaults shown)
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=4096

# ==================================================
# XAI (Grok) Provider Configuration
# ==================================================

# Required when PROVIDER=xai
XAI_API_KEY=your_xai_api_key_here

# Optional XAI settings (defaults shown)
XAI_BASE_URL=https://api.x.ai/v1
XAI_MODEL=grok-beta
XAI_MAX_TOKENS=128000

# ==================================================
# Google Gemini Provider Configuration
# ==================================================

# Required when PROVIDER=gemini (supports both environment variable names)
GEMINI_API_KEY=your_gemini_api_key_here
# GOOGLE_API_KEY=your_google_api_key_here

# Optional Gemini settings (defaults shown)
GEMINI_BASE_URL=https://generativelanguage.googleapis.com
GEMINI_MODEL=gemini-2.0-flash
GEMINI_MAX_TOKENS=32768

# ==================================================
# Mistral AI Provider Configuration
# ==================================================

# Required when PROVIDER=mistral
MISTRAL_API_KEY=your_mistral_api_key_here

# Optional Mistral settings (defaults shown)
MISTRAL_BASE_URL=https://api.mistral.ai/v1
MISTRAL_MODEL=mistral-large-latest
MISTRAL_MAX_TOKENS=32768

# ==================================================
# Ollama Provider Configuration (Local Models)
# ==================================================

# Required when PROVIDER=ollama
OLLAMA_MODEL=llama3.2

# Optional Ollama settings (defaults shown)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MAX_TOKENS=4096
OLLAMA_API_KEY=ollama

# ==================================================
# Logging Configuration
# ==================================================

# Log level: debug, info, warn, error (default: info)
LOG_LEVEL=info

# Log format: json, text (default: json)
LOG_FORMAT=json

# ==================================================
# Usage Instructions
# ==================================================

# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Choose your provider by setting PROVIDER to one of:
#    groq, openrouter, openai, xai, gemini, mistral, ollama
#
# 3. Set the corresponding API key:
#    - For Groq: Set GROQ_API_KEY
#    - For OpenRouter: Set OPENROUTER_API_KEY
#    - For OpenAI: Set OPENAI_API_KEY
#    - For XAI: Set XAI_API_KEY
#    - For Gemini: Set GEMINI_API_KEY (or GOOGLE_API_KEY)
#    - For Mistral: Set MISTRAL_API_KEY
#    - For Ollama: Set OLLAMA_MODEL (API key not required for local)
#
# 4. Adjust other settings as needed
#
# 5. Run the server:
#    go run ./cmd/proxy
#    or
#    ./bin/ccproxy-<platform>
#
# 6. Set Claude Code to use this proxy:
#    export ANTHROPIC_BASE_URL=http://localhost:7187
#    export ANTHROPIC_API_KEY=NOT_NEEDED
#    claude

# ==================================================
# Example Configurations
# ==================================================

# Example 1: Using Groq with Kimi K2
# PROVIDER=groq
# GROQ_API_KEY=gsk_your_key_here

# Example 2: Using OpenRouter with GPT-4o
# PROVIDER=openrouter  
# OPENROUTER_API_KEY=sk-or-v1-your_key_here
# OPENROUTER_MODEL=openai/gpt-4o

# Example 3: Using OpenRouter with Claude
# PROVIDER=openrouter
# OPENROUTER_API_KEY=sk-or-v1-your_key_here
# OPENROUTER_MODEL=anthropic/claude-3-sonnet

# Example 4: Using OpenRouter with Gemini
# PROVIDER=openrouter
# OPENROUTER_API_KEY=sk-or-v1-your_key_here
# OPENROUTER_MODEL=google/gemini-2.5-pro-preview

# Example 5: Using OpenAI directly
# PROVIDER=openai
# OPENAI_API_KEY=sk-your_openai_key_here
# OPENAI_MODEL=gpt-4o

# Example 6: Using XAI (Grok)
# PROVIDER=xai
# XAI_API_KEY=xai-your_key_here
# XAI_MODEL=grok-beta

# Example 7: Using Google Gemini directly
# PROVIDER=gemini
# GEMINI_API_KEY=your_gemini_key_here
# GEMINI_MODEL=gemini-2.0-flash

# Example 8: Using Mistral AI
# PROVIDER=mistral
# MISTRAL_API_KEY=your_mistral_key_here
# MISTRAL_MODEL=mistral-large-latest

# Example 9: Using Ollama (Local)
# PROVIDER=ollama
# OLLAMA_MODEL=llama3.2
# OLLAMA_BASE_URL=http://localhost:11434

# ==================================================
# Available Models
# ==================================================

# Popular Groq Models:
# - moonshotai/kimi-k2-instruct (default)
# - llama-3.1-405b-reasoning
# - llama-3.1-70b-versatile
# - mixtral-8x7b-32768

# Popular OpenRouter Models:
# - openai/gpt-4o (default)
# - openai/gpt-4-turbo
# - anthropic/claude-3-sonnet
# - anthropic/claude-3-haiku
# - google/gemini-2.5-pro-preview
# - meta-llama/llama-3.1-405b-instruct
# - mistralai/mixtral-8x7b-instruct
# - cohere/command-r-plus

# OpenAI Models:
# - gpt-4o (default)
# - gpt-4o-mini
# - gpt-4-turbo
# - gpt-3.5-turbo

# XAI (Grok) Models:
# - grok-beta (default)
# - grok-2-1212
# - grok-2-vision-1212

# Google Gemini Models:
# - gemini-2.0-flash (default)
# - gemini-1.5-pro
# - gemini-1.5-flash

# Mistral AI Models:
# - mistral-large-latest (default)
# - mistral-medium-latest
# - mistral-small-latest
# - ministral-3b-latest
# - ministral-8b-latest
# - codestral-latest

# Ollama Models (locally installed):
# - llama3.2 (default)
# - llama3.1
# - mistral
# - codellama
# - deepseek-coder
# - phi3
# - qwen2.5

# For a complete list of OpenRouter models, visit:
# https://openrouter.ai/models
#
# For Ollama models, visit:
# https://ollama.ai/library